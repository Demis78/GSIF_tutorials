library(rgdal)
library(RCurl)
MCD12Q1 <- "http://e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.051/"
options(download.file.method="auto")
items <- strsplit(getURL(MCD12Q1), "\n")[[1]]
items <- strsplit(getURL(MCD12Q1), "\n")[[1]]
?getURL
items <- strsplit(getURL(MCD12Q1), "\n")[[1]]
curl <- getCurlHandle()
options(RCurlOptions = list(capath = system.file("CurlSSL", "cacert.pem", package = "RCurl"), ssl.verifypeer = FALSE))
curlSetOpt(.opts = list(proxy = 'proxyserver:port'), curl = curl)
items
items <- getURL(MCD12Q1)
htmlTreeParse(items, asText = TRUE)
library(XML)
htmlTreeParse(items, asText = TRUE)
items
?htmlTreeParse
years <- getNodeSet(xmlParse(items), "//a")
years <- getNodeSet(xmlParse(items), "/doc//a[@href]")
years <- getNodeSet(items, "/doc//a[@href]")
xmlGetAttr(xmlParse(items), "a")
xmlParse(items)
str(items)
xmlGetAttr(htmlTreeParse(items, asText = TRUE), "a")
years <- getNodeSet(htmlTreeParse(items, asText = TRUE), "/doc//a[@href]")
years <- getNodeSet(htmlTreeParse(items), "/doc//a[@href]")
htmlTreeParse(items)
doc <- htmlTreeParse(items, asText = TRUE)
xmlGetAttr(xmlRoot(doc), "a")
years <- getNodeSet(xmlRoot(doc), "/doc//a[@href]")
years
xmlRoot(doc)
years <- getNodeSet(xmlRoot(doc), "//a[@href]")
getNodeSet(xmlRoot(doc), "//a[@href]")
years <- sapply(getNodeSet(xmlRoot(doc), "//a[@href]"), xmlGetAttr, "href")
years
years <- years[-(1:5)]
years
i=2
items.i <- getURL(paste(MCD12Q1, years[i], "/", sep=""))
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
str(nn)
nn[1:20]
nn <- nn[-(1:5)]
years[1]
paste(MCD12Q1, years[i], "/", sep="")
getwd()
j=1
is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)
is.na(file.info(paste(getwd(), years[i], sep="/"))$size)
years[i]
is.na(file.info(paste(getwd(), years[1], sep="/"))$size)
file.info(paste(getwd(), years[1], sep="/")
)
file.exists(paste(getwd(), years[1], sep="/"))
paste(getwd(), years[1], sep="/")
tt <- list.dirs(getwd())
tt
file.exists(paste(getwd(), years[4], sep="/"))
paste(getwd(), years[4], sep="/")
any(paste(getwd(), years[i], sep="/") %in% tt)
any(paste(getwd(), years[4], sep="/") %in% tt)
paste(getwd(), years[4], sep="/") %in% tt
paste(getwd(), years[4], sep="/") %in% tt
tt
paste(getwd(), years[4], sep="/")
?%in%
any(tt %in% paste(getwd(), years[i], sep="/"))
tt %in% paste(getwd(), years[i], sep="/")
tt %in% paste(getwd(), years[4], sep="/")
tt
paste(getwd(), years[4], sep="/")
any(paste(tt, "/", sep="") %in% paste(getwd(), years[i], "/"))
any(paste(tt, "/", sep="") %in% paste(getwd(), years[4], "/"))
paste(tt, "/", sep="") %in% paste(getwd(), years[4], "/")
paste(tt, "/", sep="") %in% paste(getwd(), years[4], "/")paste(getwd(), years[4], "/")
paste(getwd(), years[4], "/")
paste(tt, "/", sep="") %in% paste(getwd(), years[4], sep="/")
any(paste(tt, "/", sep="") %in% paste(getwd(), years[4], sep="/"))
!any(paste(tt, "/", sep="") %in% paste(getwd(), years[i], sep="/"))
i
paste(getwd(), years[i])
for(i in 1:length(years)){
if(!any(paste(tt, "/", sep="") %in% paste(getwd(), years[i], sep="/"))){
dir.create(paste(getwd(), years[i], sep="/"), showWarnings = TRUE, recursive = FALSE, mode = "0777")
}
}
str(nn)
View(nn)
j=1
is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T, cacheOK=FALSE)
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget')
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""))
http://e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.051/2002.01.01/MCD12Q1.A2002001.h00v08.051.2012157223436.hdf
i
i=1
j=1
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget')
http://e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.051/2001.01.01/MCD12Q1.A2001001.h00v08.051.2012157220450.hdf
paste(MCD12Q1, years[i], nn[j], sep="")
years
i
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:5)]
str(nn)
j
i
is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
j=2
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""))
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:5)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
## download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""))
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
}
}
}
gc()
options(download.file.method="auto")
i
j
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb')
j=119
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='curl')
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb')
curl <- getCurlHandle()
options(RCurlOptions = list(capath = system.file("CurlSSL", "cacert.pem", package = "RCurl"), ssl.verifypeer = FALSE))
curlSetOpt(.opts = list(proxy = 'proxyserver:port'), curl = curl)
install.packages("httr")
?httr
library(httr)
?httr
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:5)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb')
## download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
}
}
}
?try
options(show.error.messages = FALSE)
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:5)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
## download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
}
}
}
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:5)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
## download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
}
}
}
q()
library(rgdal)
library(RCurl)
library(XML)
curl <- getCurlHandle()
options(RCurlOptions = list(capath = system.file("CurlSSL", "cacert.pem", package = "RCurl"), ssl.verifypeer = FALSE))
curlSetOpt(.opts = list(proxy = 'proxyserver:port'), curl = curl)
## location of the MODIS 1 km blocks:
MCD12Q1 <- "http://e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.051/"
options(download.file.method="wget")
# get the list of directories (thanks to Barry Rowlingson):
items <- getURL(MCD12Q1)
doc <- htmlTreeParse(items, asText = TRUE)
years <- sapply(getNodeSet(xmlRoot(doc), "//a[@href]"), xmlGetAttr, "href")
years <- years[-(1:5)]
tt <- list.dirs(getwd())
for(i in 1:length(years)){
if(!any(paste(tt, "/", sep="") %in% paste(getwd(), years[i], sep="/"))){
dir.create(paste(getwd(), years[i], sep="/"), showWarnings = TRUE, recursive = FALSE, mode = "0777")
}
}
options(show.error.messages = FALSE)
## 634 blocks per year
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:5)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
## download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
}
}
}
years <- sapply(getNodeSet(xmlRoot(doc), "//a[@href]"), xmlGetAttr, "href")
years
years <- years[-(1:7)]
tt <- list.dirs(getwd())
for(i in 1:length(years)){
if(!any(paste(tt, "/", sep="") %in% paste(getwd(), years[i], sep="/"))){
dir.create(paste(getwd(), years[i], sep="/"), showWarnings = TRUE, recursive = FALSE, mode = "0777")
}
}
## 634 blocks per year
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:5)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
##try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
}
}
}
i=1
items.i <- getURL(paste(MCD12Q1, years[i], sep="")) ## takes 2 mins
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:7)]
nn
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn[1:8]
nn <- nn[-(1:7)]
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep=""))
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:7)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
##try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb', method='wget', quiet=T)
}
}
}
library(rgdal)
library(RCurl)
library(XML)
curl <- getCurlHandle()
options(RCurlOptions = list(capath = system.file("CurlSSL", "cacert.pem", package = "RCurl"), ssl.verifypeer = FALSE))
curlSetOpt(.opts = list(proxy = 'proxyserver:port'), curl = curl)
## location of the MODIS 1 km blocks:
MCD12Q1 <- "http://e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.051/"
options(download.file.method="wget")
# get the list of directories (thanks to Barry Rowlingson):
items <- getURL(MCD12Q1)
doc <- htmlTreeParse(items, asText = TRUE)
years <- sapply(getNodeSet(xmlRoot(doc), "//a[@href]"), xmlGetAttr, "href")
years <- years[-(1:7)]
tt <- list.dirs(getwd())
for(i in 1:length(years)){
if(!any(paste(tt, "/", sep="") %in% paste(getwd(), years[i], sep="/"))){
dir.create(paste(getwd(), years[i], sep="/"), showWarnings = TRUE, recursive = FALSE, mode = "0777")
}
}
## 634 blocks per year
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep=""))
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:7)]
for(j in 1:length(nn)){
if(is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)){
##try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), method='wget')
}
}
}
options(download.file.method="auto")
i=1
j=234
items.i <- getURL(paste(MCD12Q1, years[i], sep=""))
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:7)]
is.na(file.info(paste(getwd(), years[i], nn[j], sep="/"))$size)
file.info(paste(getwd(), years[i], nn[j], sep="/"))$size
## 634 blocks per year
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep=""))
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:7)]
for(j in 1:length(nn)){
fileT = file.info(paste(getwd(), years[i], nn[j], sep="/"))$size
if(is.na(fileT)|fileT==0){
##try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), method='wget')
}
}
}
options(download.file.method="wget")
?download.file
download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), method='wget', extra='http_proxy_user=tomhengl:.lEGA2005')
paste(MCD12Q1, years[i], nn[j], sep="")
download.file(gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep="")), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), method='wget', extra='http_proxy_user=tomhengl:.lEGA2005')
gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep=""))
download.file(gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep="")), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), method='auto')
options(download.file.method="wget")
download.file(gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep="")), destfile=paste(getwd(), "/", years[i], nn[j], sep=""))
options(download.file.method="auto")
download.file(gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep="")), destfile=paste(getwd(), "/", years[i], nn[j], sep=""))
gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep=""))
system('wget http://tomhengl:.lEGA2005@e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.051/2001.01.01/MCD12Q1.A2001001.h01v09.051.2014287161544.hdf')
setInternet2(TRUE)
download.file(gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep="")), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE)
download.file(gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep="")), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget")
wf = gsub('http://', 'http://tomhengl:.lEGA2005@', paste(MCD12Q1, years[i], nn[j], sep=""))
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget")
Sys.setenv(http_proxy_user='tomhengl:.lEGA2005')
wf = paste(MCD12Q1, years[i], nn[j], sep="")
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget")
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="auto")
Sys.setenv()
Sys.setenv(http_proxy_user)
Sys.setenv(http_proxy_user='tomhengl:.lEGA2005')
Sys.getenv("http_proxy_user")
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="internal")
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="libcurl")
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="curl")
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep=""))
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:7)]
for(j in 1:length(nn)){
fileT = file.info(paste(getwd(), years[i], nn[j], sep="/"))$size
if(is.na(fileT)|fileT==0){
##try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
wf = paste(MCD12Q1, years[i], nn[j], sep="")
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="curl")
}
}
}
Sys.setenv(http_proxy_user='tomhengl:.lEGA2005')
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget", mode="wb", extra='-L --load-cookies ~/.cookies --save-cookies ~/.cookies')
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget", mode="wb", extra='--username:tomhengl --password:.lEGA2005')
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget", mode="wb", extra='--username tomhengl --password .lEGA2005')
system('wget --help')
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget", mode="wb", extra='--http-user=tomhengl --http-password=.lEGA2005')
for(i in 1:length(years)){
items.i <- getURL(paste(MCD12Q1, years[i], sep=""))
doc.i <- htmlTreeParse(items.i, asText = TRUE)
nn <- sapply(getNodeSet(xmlRoot(doc.i), "//a[@href]"), xmlGetAttr, "href")
nn <- nn[-(1:7)]
for(j in 1:length(nn)){
fileT = file.info(paste(getwd(), years[i], nn[j], sep="/"))$size
if(is.na(fileT)|fileT==0){
##try(download.file(paste(MCD12Q1, years[i], nn[j], sep=""), destfile=paste(getwd(), "/", years[i], nn[j], sep=""), mode='wb'))
wf = paste(MCD12Q1, years[i], nn[j], sep="")
download.file(wf, destfile=paste(getwd(), "/", years[i], nn[j], sep=""), cacheOK=TRUE, method="wget", mode="wb", extra='--http-user=tomhengl --http-password=.lEGA2005')
}
}
}
save.image()
setwd("/data/models/TAXOUSDA")
library(aqp)
library(plyr)
library(stringr)
library(dplyr)
library(sp)
library(devtools)
#devtools::install_github('topepo/caret/pkg/caret')
library(caret)
#install_bitbucket("mkuhn/parallelRandomForest", ref="parallelRandomForest")
#library(parallelRandomForest)
#library(e1071)
#library(randomForest)
#devtools::install_github("imbs-hl/ranger/ranger-r-package/ranger", ref="forest_memory") ## version to deal with Memory problems
library(ranger)
library(nnet)
library(dplyr)
library(ROCR)
library(snowfall)
library(mda)
library(psych)
library(rgdal)
library(utils)
library(plotKML)
library(GSIF)
writeOGR(xy.pnts["TAXOUSDA.f"], "TAXOUSDA_observed.gpkg", "TAXOUSDA_observed",  driver = "GPKG")
load()
load(".RData")
writeOGR(xy.pnts["TAXOUSDA.f"], "TAXOUSDA_observed.gpkg", "TAXOUSDA_observed",  driver = "GPKG")
setwd("/data/models/TAXNWRB")
load(".RData")
writeOGR(xy.pnts["TAXNWRB.f"], "TAXNWRB_observed.gpkg", "TAXNWRB_observed",  driver = "GPKG")
library(entropy)
p=c(0,0.8,0.2,0,0)
entropy.empirical(p)/entropy.empirical(rep(1/length(p),length(p)))
p=c(0,0.7,0.3,0,0)
entropy.empirical(p)/entropy.empirical(rep(1/length(p),length(p)))
p=c(0,0.7,0.2,0.1,0)
entropy.empirical(p)/entropy.empirical(rep(1/length(p),length(p)))
setwd("~/Documents/git/GSIF_tutorials/eberg")
list.of.packages <- c("nnet", "plyr", "ROCR", "randomForest", "plyr", "parallel", "psych", "mda", "Cubist", "h2o", "dismo", "grDevices", "snowfall", "hexbin", "lattice", "ranger", "xgboost", "mxnet", "doParallel", "caret")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
#library(caretEnsemble)
library(plotKML)
library(sp)
library(randomForest)
library(nnet)
library(e1071)
library(GSIF)
library(plyr)
library(raster)
library(caret)
library(Cubist)
library(GSIF)
library(xgboost)
library(snowfall)
#library(devtools)
#devtools::install_github('dmlc/mxnet/R-package')
#https://github.com/dmlc/dmlc-core/issues/144
library(mxnet)
data(eberg)
data(eberg_grid)
coordinates(eberg) <- ~X+Y
proj4string(eberg) <- CRS("+init=epsg:31467")
gridded(eberg_grid) <- ~x+y
proj4string(eberg_grid) <- CRS("+init=epsg:31467")
eberg_spc <- spc(eberg_grid, ~ PRMGEO6+DEMSRT6+TWISRT6+TIRAST6)
eberg_grid@data <- cbind(eberg_grid@data, eberg_spc@predicted@data)
## overlay and create a regression-matrix:
ov <- over(eberg, eberg_grid)
m <- cbind(ov, eberg@data)
dim(m)
# ------------------------------------------------------------
# SOIL TYPES / FACTORS
# ------------------------------------------------------------
## clean-up target variable:
xg = summary(m$TAXGRSC, maxsum=length(levels(m$TAXGRSC)))
str(xg)
selg.levs = attr(xg, "names")[xg > 5]
m$soiltype <- m$TAXGRSC
m$soiltype[which(!m$TAXGRSC %in% selg.levs)] <- NA
m$soiltype <- droplevels(m$soiltype)
str(summary(m$soiltype, maxsum=length(levels(m$soiltype))))
## use only complete cases:
m <- m[complete.cases(m[,1:(ncol(eberg_grid)+2)]),]
m$soiltype <- as.factor(m$soiltype)
## subsample
s <- sample.int(nrow(m), 500)
## CV error for RF
TAXGRSC.rf <- randomForest(x=m[-s,paste0("PC",1:10)], y=m$soiltype[-s], xtest=m[s,paste0("PC",1:10)], ytest=m$soiltype[s])
TAXGRSC.rf$test$confusion[,"class.error"]
## Fit 4 independent machine learning models
## Random Forest
TAXGRSC.rf <- randomForest(x=m[,paste0("PC",1:10)], y=m$soiltype, keep.forest = TRUE)
## prediction success per class:
TAXGRSC.rf
## Multinomial Logistic Regression
fm = as.formula(paste("soiltype~", paste(paste0("PC",1:10), collapse="+")))
TAXGRSC.mn <- multinom(fm, m)
## Support Vector Machines
TAXGRSC.svm <- svm(fm, m, probability=TRUE, cross=5) ## Takes few secs
TAXGRSC.svm$tot.accuracy
## Deep Neural Net
train.x = data.matrix(m[,all.vars(fm)[-1]])
## mxnet requires that the classes are coded as 0, 1, 2 ... p
train.yf = as.factor(as.vector(m[,all.vars(fm)[1]]))
train.y = as.integer(train.yf)-1
TAXGRSC.nn <- mx.mlp(train.x, train.y, hidden_node=ncol(train.x), out_node=length(levels(m$soiltype)), out_activation="softmax", num.round=10, array.batch.size=15, learning.rate=0.1, momentum=0.9, eval.metric=mx.metric.accuracy, array.layout="rowmajor")
## Make ensemb
probs1 <- predict(TAXGRSC.mn, eberg_grid@data, type="probs", na.action = na.pass)
probs2 <- predict(TAXGRSC.rf, eberg_grid@data, type="prob", na.action = na.pass)
probs3 <- attr(predict(TAXGRSC.svm, eberg_grid@data, probability=TRUE, na.action = na.pass), "probabilities")
probs4 <- t(predict(TAXGRSC.nn, data.matrix(eberg_grid@data[,all.vars(fm)[-1]]), array.layout="rowmajor"))
attr(probs4, "dimnames")[[2]] = levels(train.yf)
eberg_soiltype@data <- data.frame(probs4)
eberg_soiltype = eberg_grid
eberg_soiltype@data <- data.frame(probs4)
plot(raster::stack(eberg_soiltype), col=SAGA_pal[[1]], zlim=c(0,1))
plot(raster::stack(eberg_soiltype), col=SAGA_pal[[1]], zlim=c(0,1))
dev.off()
dev.off()
plot(raster::stack(eberg_soiltype), col=SAGA_pal[[1]], zlim=c(0,1))
